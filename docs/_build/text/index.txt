
Welcome to hadoop-manager's documentation!
******************************************

Contents:


hadoop-manager
==============

Python wrapper around Hadoop streaming jar.

class class hdpmanager.HadoopManager(hadoop_home, hadoop_fs_default_name=None, hadoop_job_tracker=None)

   HadoopManager is a central object for managing hadoop jobs and hdfs

   Parameters:
      * **hadoop_home** -- home folder of hadoop package

      * **hadoop_fs_default_name** -- default hdfs home used when
        paths provided are relative

      * **hadoop_job_tracker** -- hadoop job tracker host:port

   create_job(input_paths, output_path, mapper, reducer=None, combiner=None, num_reducers=None, serialization=None, job_env=None, conf=None, root_package=None)

      Create HadoopJob object

      Parameters:
         * **input_paths** -- list of input files for mapper

         * **output_path** -- path to the output dir

         * **mapper** -- import path to the mapper class

         * **reducer** -- import path to the reducer class

         * **combiner** -- import path to the combiner class

         * **root_package** -- import path to the subpackage in you
           app where the mapper/reducer/combiner import starts

         * **num_reducers** -- number of reducers

         * **conf** -- object that will be send to mapper, reducer
           and combiner. It will be accessible as self.conf in job
           objects.

         * **serialization** -- dict with configuration for input,
           output and internal serialization. Valid keys are input,
           output and inter, valid values are json, pickle and raw.

         * **job_env** -- dict which defines environment. Valid
           options are packages, package_data and requires. If
           packages aren't provided all packages returned by
           setuptools.find_packages in root_package will be included

   fs

      HadoopFs object for managing hdfs

class class hdpmanager.HadoopJob(hdp_manager, input_paths, output_path, mapper, reducer=None, combiner=None, num_reducers=None, serialization=None, job_env=None, conf=None, root_package=None)

   HadoopJob object for managing mapreduce jobs Create it with the
   HadoopManager.create_job methos

   cat_output()

      Returns a generator over mapreduce output

   rm_output()

      Remove output dir

   run()

      Run a mapreduce job

   run_local()

      Run job in a local simulated environment

class class hdpmanager.HadoopFs(hadoop_manager)

   cat(path, serializer='raw', tab_separated=False)

      Returns a generator over files defined by path

      Parameters:
         * **path** -- path to the files

         * **serializer** -- input serializer. Options are json,
           pickle and raw(default)

         * **tab_seperated** -- boolean if input is tab separated

   rm(path)

      Recursively remove all files on the path

      Parameters:
         **path** -- path to the files

class class hdpmanager.Mapper(input_stream=<open file '<stdin>', mode 'r' at 0x2b83e4a4e0c0>, output_stream=<open file '<stdout>', mode 'w' at 0x2b83e4a4e150>)

   map(line)

      Override this methos for mapping the input line Output can be
      either returned or yielded as a key, value pair

      Parameters:
         **line** -- one line of the input file serialized by the
         input serializer

class class hdpmanager.Reducer(input_stream=<open file '<stdin>', mode 'r' at 0x2b83e4a4e0c0>, output_stream=<open file '<stdout>', mode 'w' at 0x2b83e4a4e150>)

   reduce(key, values)

      Override this methos for reducing the input Output can be either
      returned or yielded as a key, value pair

      Parameters:
         * **key** -- key returned by the mapper

         * **values** -- list of values retuned by the mapper

class class hdpmanager.Combiner(input_stream=<open file '<stdin>', mode 'r' at 0x2b83e4a4e0c0>, output_stream=<open file '<stdout>', mode 'w' at 0x2b83e4a4e150>)

   reduce(key, values)

      Override this methos for reducing the input Output can be either
      returned or yielded as a key, value pair

      Parameters:
         * **key** -- key returned by the mapper

         * **values** -- list of values retuned by the mapper


Indices and tables
******************

* *Index*

* *Module Index*

* *Search Page*
